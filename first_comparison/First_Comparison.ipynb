{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import psutil\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "1qDHs6EWUVJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(model_name, user_input):\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cpu\")\n",
        "    summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    prompt = f\"summarize: {user_input}\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    mem_before = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "\n",
        "    result = summarizer(prompt, max_length=100, min_length=20, do_sample=False)\n",
        "\n",
        "    mem_after = psutil.Process().memory_info().rss / 1024 / 1024\n",
        "    end_time = time.time()\n",
        "\n",
        "    return {\n",
        "        \"output\": result[0]['generated_text'],\n",
        "        \"time_taken_sec\": round(end_time - start_time, 2),\n",
        "        \"memory_used_mb\": round(mem_after - mem_before, 2)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "tk_guzYUWxmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Get user input ===\n",
        "user_input = input(\"Enter the paragraph you want to summarize:\\n\")\n"
      ],
      "metadata": {
        "id": "r1tR4TybW4Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Define models ===\n",
        "slm_model = \"google/flan-t5-small\"   # Small Language Model (~80M params)\n",
        "llm_model = \"google/flan-t5-xl\"      # Large Language Model (~3B params)\n",
        "\n"
      ],
      "metadata": {
        "id": "DHSk3Xr7W62V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run SLM ===\n",
        "print(\"\\n--- SLM: google/flan-t5-small ---\")\n",
        "slm_result = run_model(slm_model, user_input)\n",
        "print(\"Summary:\", slm_result[\"output\"])\n",
        "print(\"Time (s):\", slm_result[\"time_taken_sec\"])\n",
        "print(\"Memory Used (MB):\", slm_result[\"memory_used_mb\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "r3DC7EWiW9tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run LLM ===\n",
        "print(\"\\n--- LLM: google/flan-t5-xl ---\")\n",
        "llm_result = run_model(llm_model, user_input)\n",
        "print(\"Summary:\", llm_result[\"output\"])\n",
        "print(\"Time (s):\", llm_result[\"time_taken_sec\"])\n",
        "print(\"Memory Used (MB):\", llm_result[\"memory_used_mb\"])"
      ],
      "metadata": {
        "id": "NbxisgE_W_Q8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}